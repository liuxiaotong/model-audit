"""ModelAudit MCP Server â€” Model Context Protocol æœåŠ¡."""

from typing import Any

try:
    from mcp.server import Server
    from mcp.server.stdio import stdio_server
    from mcp.types import TextContent, Tool

    HAS_MCP = True
except ImportError:
    HAS_MCP = False

from modelaudit.config import AuditConfig
from modelaudit.engine import AuditEngine
from modelaudit.report import generate_report


def create_server() -> "Server":
    """åˆ›å»º MCP æœåŠ¡å™¨å®ä¾‹."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install knowlyr-modelaudit[mcp]")

    server = Server("modelaudit")

    @server.list_tools()
    async def list_tools() -> list[Tool]:
        """åˆ—å‡ºå¯ç”¨çš„å·¥å…·."""
        return [
            Tool(
                name="detect_text_source",
                description="æ£€æµ‹æ–‡æœ¬æ•°æ®æ¥æº â€” åˆ¤æ–­æ–‡æœ¬å¯èƒ½ç”±å“ªä¸ª LLM ç”Ÿæˆ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "texts": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å¾…æ£€æµ‹çš„æ–‡æœ¬åˆ—è¡¨",
                        },
                    },
                    "required": ["texts"],
                },
            ),
            Tool(
                name="verify_model",
                description="éªŒè¯æ¨¡å‹èº«ä»½ â€” æ£€æŸ¥ API èƒŒåæ˜¯ä¸æ˜¯å£°ç§°çš„æ¨¡å‹",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "model": {
                            "type": "string",
                            "description": "æ¨¡å‹åç§° (å¦‚ gpt-4o, claude-3-opus)",
                        },
                        "provider": {
                            "type": "string",
                            "enum": ["openai", "anthropic", "custom"],
                            "description": "API æä¾›å•† (é»˜è®¤: openai)",
                        },
                    },
                    "required": ["model"],
                },
            ),
            Tool(
                name="compare_models",
                description="æ¯”å¯¹ä¸¤ä¸ªæ¨¡å‹çš„æŒ‡çº¹ç›¸ä¼¼åº¦ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨è’¸é¦/æ´¾ç”Ÿå…³ç³»",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "model_a": {"type": "string", "description": "æ¨¡å‹ A åç§°"},
                        "model_b": {"type": "string", "description": "æ¨¡å‹ B åç§°"},
                        "provider": {
                            "type": "string",
                            "description": "API æä¾›å•† (é»˜è®¤: openai)",
                        },
                        "method": {
                            "type": "string",
                            "enum": ["llmmap", "dli", "style"],
                            "description": "æŒ‡çº¹æ–¹æ³• (é»˜è®¤: llmmap)",
                        },
                    },
                    "required": ["model_a", "model_b"],
                },
            ),
            Tool(
                name="compare_models_whitebox",
                description="ç™½ç›’æ¯”å¯¹ä¸¤ä¸ªæœ¬åœ°æ¨¡å‹ â€” ä½¿ç”¨ REEF CKA æ–¹æ³•æ¯”è¾ƒæ¨¡å‹éšè—çŠ¶æ€ç›¸ä¼¼åº¦ï¼ˆéœ€è¦æ¨¡å‹æƒé‡ï¼‰",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "model_a": {
                            "type": "string",
                            "description": "æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹å A",
                        },
                        "model_b": {
                            "type": "string",
                            "description": "æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹å B",
                        },
                        "device": {
                            "type": "string",
                            "description": "è®¡ç®—è®¾å¤‡ (é»˜è®¤: cpu)",
                        },
                    },
                    "required": ["model_a", "model_b"],
                },
            ),
            Tool(
                name="audit_distillation",
                description=(
                    "å®Œæ•´è’¸é¦å®¡è®¡ â€” ç»¼åˆæŒ‡çº¹æ¯”å¯¹ + é£æ ¼åˆ†æï¼Œç”Ÿæˆè¯¦ç»†å®¡è®¡æŠ¥å‘Šã€‚"
                    "æ”¯æŒè·¨ provider å®¡è®¡ï¼ˆå¦‚ Kimi API + Claude APIï¼‰ã€‚"
                ),
                inputSchema={
                    "type": "object",
                    "properties": {
                        "teacher": {
                            "type": "string",
                            "description": "æ•™å¸ˆæ¨¡å‹ (ç–‘ä¼¼è¢«è’¸é¦çš„æºæ¨¡å‹)",
                        },
                        "student": {
                            "type": "string",
                            "description": "å­¦ç”Ÿæ¨¡å‹ (ç–‘ä¼¼è’¸é¦äº§ç‰©)",
                        },
                        "teacher_provider": {
                            "type": "string",
                            "description": "æ•™å¸ˆæ¨¡å‹ API æä¾›å•† (é»˜è®¤: openai)",
                        },
                        "student_provider": {
                            "type": "string",
                            "description": "å­¦ç”Ÿæ¨¡å‹ API æä¾›å•† (é»˜è®¤: openai)",
                        },
                        "teacher_api_base": {
                            "type": "string",
                            "description": "æ•™å¸ˆæ¨¡å‹è‡ªå®šä¹‰ API åœ°å€",
                        },
                        "student_api_base": {
                            "type": "string",
                            "description": "å­¦ç”Ÿæ¨¡å‹è‡ªå®šä¹‰ API åœ°å€",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "json"],
                            "description": "æŠ¥å‘Šæ ¼å¼ (é»˜è®¤: markdown)",
                        },
                    },
                    "required": ["teacher", "student"],
                },
            ),
            Tool(
                name="audit_memorization",
                description="æ£€æµ‹æ¨¡å‹æ˜¯å¦è®°å¿†äº†è®­ç»ƒæ•°æ® â€” é€šè¿‡å‰ç¼€è¡¥å…¨å’Œé€å­—æ£€æŸ¥è¯„ä¼°è®°å¿†ç¨‹åº¦",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "text_samples": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å€™é€‰è®­ç»ƒæ–‡æœ¬åˆ—è¡¨",
                        },
                        "model": {
                            "type": "string",
                            "description": "å¾…æµ‹æ¨¡å‹åç§° (é»˜è®¤: gpt-4o)",
                        },
                        "provider": {
                            "type": "string",
                            "enum": ["openai", "anthropic"],
                            "description": "API æä¾›å•† (é»˜è®¤: openai)",
                        },
                        "method": {
                            "type": "string",
                            "enum": ["prefix_completion", "verbatim_check", "both"],
                            "description": "æ£€æµ‹æ–¹æ³• (é»˜è®¤: prefix_completion)",
                        },
                    },
                    "required": ["text_samples"],
                },
            ),
            Tool(
                name="audit_report",
                description="ç”Ÿæˆå®Œæ•´çš„æ¨¡å‹å®¡è®¡æŠ¥å‘Š â€” æ±‡æ€»æ‰€æœ‰å®¡è®¡å·¥å…·çš„ç»“æœ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "results": {
                            "type": "object",
                            "description": "å„å®¡è®¡å·¥å…·çš„ç»“æœå­—å…¸ (tool_name -> result_text)",
                        },
                        "model_name": {
                            "type": "string",
                            "description": "è¢«å®¡è®¡æ¨¡å‹åç§°",
                        },
                        "audit_date": {
                            "type": "string",
                            "description": "å®¡è®¡æ—¥æœŸ (é»˜è®¤: ä»Šå¤©)",
                        },
                    },
                    "required": ["results", "model_name"],
                },
            ),
        ]

    @server.call_tool()
    async def call_tool(name: str, arguments: dict[str, Any]) -> list[TextContent]:
        """è°ƒç”¨å·¥å…·."""

        if name == "detect_text_source":
            texts = arguments["texts"]
            engine = AuditEngine()
            results = engine.detect(texts)

            lines = ["## æ–‡æœ¬æ¥æºæ£€æµ‹ç»“æœ", ""]
            lines.append("| # | é¢„æµ‹æ¨¡å‹ | ç½®ä¿¡åº¦ | é¢„è§ˆ |")
            lines.append("|---|---------|--------|------|")

            for r in results:
                lines.append(
                    f"| {r.text_id} | {r.predicted_model} | {r.confidence:.2%} | {r.text_preview} |"
                )

            # ç»Ÿè®¡
            model_counts: dict[str, int] = {}
            for r in results:
                model_counts[r.predicted_model] = model_counts.get(r.predicted_model, 0) + 1

            lines.extend(["", "### æ¥æºåˆ†å¸ƒ"])
            for model, count in sorted(model_counts.items(), key=lambda x: -x[1]):
                pct = count / len(results) * 100
                lines.append(f"- {model}: {count} ({pct:.1f}%)")

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "verify_model":
            model = arguments["model"]
            provider = arguments.get("provider", "openai")

            config = AuditConfig(provider=provider)
            engine = AuditEngine(config)
            result = engine.verify(model, provider=provider)

            verified = result["verified"]
            icon = "âœ“" if verified else "âœ—"

            text = f"""## æ¨¡å‹èº«ä»½éªŒè¯

{icon} **{'éªŒè¯é€šè¿‡' if verified else 'éªŒè¯æœªé€šè¿‡'}**

- å£°ç§°æ¨¡å‹: {model}
- æœ€ä½³åŒ¹é…: {result['best_match']} (åˆ†æ•°: {result['best_score']})

### é£æ ¼åŒ¹é…åˆ†æ•°
"""
            for name_k, score in result.get("all_scores", {}).items():
                text += f"- {name_k}: {score:.4f}\n"

            return [TextContent(type="text", text=text)]

        elif name == "compare_models":
            model_a = arguments["model_a"]
            model_b = arguments["model_b"]
            provider = arguments.get("provider", "openai")
            method = arguments.get("method", "llmmap")

            config = AuditConfig(provider=provider)
            engine = AuditEngine(config)
            result = engine.compare(model_a, model_b, method=method, provider=provider)

            derived_text = "å¯èƒ½å­˜åœ¨æ´¾ç”Ÿå…³ç³»" if result.is_derived else "å¯èƒ½æ˜¯ç‹¬ç«‹æ¨¡å‹"
            icon = "âš ï¸" if result.is_derived else "âœ“"

            text = f"""## æ¨¡å‹æ¯”å¯¹ç»“æœ

{icon} **{derived_text}**

- æ¨¡å‹ A: {model_a}
- æ¨¡å‹ B: {model_b}
- ç›¸ä¼¼åº¦: {result.similarity:.4f}
- é˜ˆå€¼: {result.threshold}
- ç½®ä¿¡åº¦: {result.confidence:.4f}
"""
            return [TextContent(type="text", text=text)]

        elif name == "compare_models_whitebox":
            model_a = arguments["model_a"]
            model_b = arguments["model_b"]
            device = arguments.get("device", "cpu")

            engine = AuditEngine(use_cache=False)
            result = engine.compare(
                model_a, model_b, method="reef", device=device,
            )

            derived_text = "å¯èƒ½å­˜åœ¨æ´¾ç”Ÿå…³ç³»" if result.is_derived else "å¯èƒ½æ˜¯ç‹¬ç«‹æ¨¡å‹"
            icon = "âš ï¸" if result.is_derived else "âœ“"

            text = f"""## ç™½ç›’æ¨¡å‹æ¯”å¯¹ç»“æœ (REEF CKA)

{icon} **{derived_text}**

- æ¨¡å‹ A: {model_a}
- æ¨¡å‹ B: {model_b}
- ç›¸ä¼¼åº¦: {result.similarity:.4f}
- é˜ˆå€¼: {result.threshold}
- ç½®ä¿¡åº¦: {result.confidence:.4f}
"""
            if "layer_cka" in result.details:
                text += "\n### é€å±‚ CKA ç›¸ä¼¼åº¦\n"
                for layer, cka in result.details["layer_cka"].items():
                    text += f"- {layer}: {cka:.4f}\n"

            return [TextContent(type="text", text=text)]

        elif name == "audit_distillation":
            teacher = arguments["teacher"]
            student = arguments["student"]
            output_format = arguments.get("format", "markdown")

            config = AuditConfig()
            engine = AuditEngine(config)
            result = engine.audit(
                teacher, student,
                teacher_provider=arguments.get("teacher_provider"),
                teacher_api_key=arguments.get("teacher_api_key"),
                teacher_api_base=arguments.get("teacher_api_base"),
                student_provider=arguments.get("student_provider"),
                student_api_key=arguments.get("student_api_key"),
                student_api_base=arguments.get("student_api_base"),
            )

            report = generate_report(result, output_format)
            return [TextContent(type="text", text=report)]

        elif name == "audit_memorization":
            from difflib import SequenceMatcher
            text_samples = arguments["text_samples"]
            model = arguments.get("model", "gpt-4o")
            provider = arguments.get("provider", "openai")
            method = arguments.get("method", "prefix_completion")

            # Try to call model API
            try:
                from modelaudit.methods.llmmap import _call_model_api_once
                has_client = True
            except ImportError:
                has_client = False

            if not has_client:
                lines = ["## è®°å¿†æ£€æµ‹ â€” äº¤äº’æ¨¡å¼", "", "æœªæ‰¾åˆ° LLM å®¢æˆ·ç«¯åº“ï¼Œä»¥ä¸‹æ˜¯ä¾›æ‰‹åŠ¨æ‰§è¡Œçš„æç¤ºï¼š", ""]
                for i, sample in enumerate(text_samples):
                    mid = len(sample) // 2
                    lines.append(f"### æ ·æœ¬ {i + 1}")
                    lines.append(f"```\nè¯·ç»­å†™ä»¥ä¸‹æ–‡æœ¬ï¼š\n\n{sample[:mid]}\n```\n")
                return [TextContent(type="text", text="\n".join(lines))]

            results = []
            for i, sample in enumerate(text_samples):
                mid = len(sample) // 2
                prefix = sample[:mid]
                suffix = sample[mid:]
                if method in ("prefix_completion", "both"):
                    prompt = f"è¯·ç»­å†™ä»¥ä¸‹æ–‡æœ¬ï¼Œç›´æ¥è¾“å‡ºç»­å†™éƒ¨åˆ†ï¼š\n\n{prefix}"
                    completion = _call_model_api_once(model, prompt, provider=provider)
                    score = SequenceMatcher(None, suffix.strip(), completion.strip()).ratio()
                    results.append({"index": i + 1, "method": "prefix_completion", "score": round(score, 4)})
                if method in ("verbatim_check", "both"):
                    prompt = f"è¯·ç”¨ä½ è‡ªå·±çš„è¯å¤è¿°ä»¥ä¸‹æ–‡æœ¬çš„å…³é”®ä¿¡æ¯ï¼š\n\n{sample[:200]}"
                    response = _call_model_api_once(model, prompt, provider=provider)
                    words = sample.split()
                    phrase_len = min(8, len(words) // 4) or 3
                    matches = total = 0
                    for j in range(0, len(words) - phrase_len + 1, phrase_len):
                        phrase = " ".join(words[j:j + phrase_len])
                        total += 1
                        if phrase in response:
                            matches += 1
                    score = matches / total if total else 0
                    results.append({"index": i + 1, "method": "verbatim_check", "score": round(score, 4)})

            lines = [f"## è®­ç»ƒæ•°æ®è®°å¿†æ£€æµ‹ç»“æœ ({model})", "", f"- æ£€æµ‹æ–¹æ³•: {method}", f"- æ ·æœ¬æ•°é‡: {len(text_samples)}", "",
                     "| # | æ–¹æ³• | è®°å¿†åˆ†æ•° | è¯„çº§ |", "|---|------|---------|------|"]
            for r in results:
                level = "high" if r["score"] >= 0.7 else ("medium" if r["score"] >= 0.4 else "low")
                icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}[level]
                lines.append(f"| {r['index']} | {r['method']} | {r['score']:.4f} | {icon} {level} |")
            if results:
                avg = sum(r["score"] for r in results) / len(results)
                lines.extend(["", f"- å¹³å‡è®°å¿†åˆ†æ•°: {avg:.4f}"])
            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "audit_report":
            from datetime import date
            results = arguments["results"]
            model_name = arguments["model_name"]
            audit_date = arguments.get("audit_date", date.today().isoformat())
            sections = {
                "detect_text_source": "æ–‡æœ¬æ¥æºæ£€æµ‹",
                "verify_model": "æ¨¡å‹èº«ä»½éªŒè¯",
                "audit_distillation": "è’¸é¦åˆ†æ",
                "compare_models": "æ¨¡å‹æŒ‡çº¹æ¯”å¯¹",
                "audit_memorization": "è®°å¿†æ£€æµ‹",
            }
            lines = [f"# æ¨¡å‹å®¡è®¡æŠ¥å‘Šï¼š{model_name}", "", f"**å®¡è®¡æ—¥æœŸ**: {audit_date}", ""]
            for key, title in sections.items():
                lines.append(f"## {title}")
                lines.append("")
                if key in results:
                    lines.append(results[key])
                else:
                    lines.append("*æœªæ‰§è¡Œæ­¤é¡¹æ£€æŸ¥ã€‚*")
                lines.append("")
            return [TextContent(type="text", text="\n".join(lines))]

        else:
            return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]

    return server


async def serve():
    """å¯åŠ¨ MCP æœåŠ¡å™¨."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install knowlyr-modelaudit[mcp]")

    server = create_server()
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream)


def main():
    """ä¸»å…¥å£."""
    import asyncio

    asyncio.run(serve())


if __name__ == "__main__":
    main()
